# Introduction

    Diffusion models have demonstrated remarkable success in various image generation tasks, 
    but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. 
    To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. 
    Our method, <strong>DiffMoE</strong>, introduces a <strong>batch-level global token pool</strong> that enables experts to access global token distributions during training, 
    promoting specialized expert behavior. To unleash the full potential of the diffusion process, 
    DiffMoE incorporates a <strong>capacity predictor</strong> that dynamically allocates computational resources based on noise levels and sample complexity. 
    Through comprehensive evaluation,  DiffMoE  achieves state-of-the-art performance among diffusion models on ImageNet benchmark, 
    substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. 
    The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications.  


# Method Overview

    DiffMoE flattens tokens into a batch-level global token pool, where each expert maintains a fixed training capacity. 
    During inference, a dynamic capacity predictor adaptively routes tokens across different sampling steps and conditions. 
    Different colors denote tokens from distinct samples, while ti represents corresponding noise levels.

    <figure style={{
    display: 'flex',
    flexDirection: 'column',
    alignItems: 'center'
    }}>
    <img src="./images/Method.png" alt="viz" height="400"  />
    <figcaption>Fig 2: DiffMoE Architecture Overview. </figcaption>
    </figure>


# Experiments

**ImageNet Generation.** After 7000K steps, DiffMoE-L-E8 achieves FID50K scores of DDPM/Flow (2.30/2.13) with cfg=1.5, surpassing Dense-DiT-XL (2.32/2.19) as shown below. All evaluations follow DiT's protocol.

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center'
}}>
  <img src="./images/SOTA.png" alt="method" height="450" />
  <figcaption>Fig 3: ImageNet Generation Results.</figcaption>
</figure>

**Scaling Parameter Behavior.** To explore the upper limits of DiffMoE and quantify its performance efficiency, we scaled the model to larger sizes and trained them for 3000K steps. DiffMoE-L-E16-Flow achieves the best performance among the evaluated models. Notably, DiffMoE-L-E16 surpasses the performance of Dense-DiT-XXXL-Flow, which uses 3x the parameters, while operating with only 1x the parameters. This highlights the exceptional parameter efficiency and scalability of DiffMoE.

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center'
}}>
  <img src="./images/SOTA-2.png" alt="method" height="300" />
  <figcaption>Fig 4: Scaling Parameter Behavior.</figcaption>
</figure>

# Visualization

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  marginBottom: '2rem'
}}>
  <img src="./images/viz-1.png" alt="visualization 1" width="600" />
</figure>

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  marginBottom: '2rem'
}}>
  <img src="./images/viz-2.png" alt="visualization 2" width="600" />
</figure>

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center',
  marginBottom: '2rem'
}}>
  <img src="./images/viz-3.png" alt="visualization 3" width="600" />
</figure>
